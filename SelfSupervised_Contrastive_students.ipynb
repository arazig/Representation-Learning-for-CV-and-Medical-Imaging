{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JdZ_NoJMUp1"
      },
      "source": [
        "## Contrastive pre-training\n",
        "\n",
        "Here, we will try the SimCLR method.\n",
        "\n",
        "[1] T. Chen et al. “A Simple Framework for Contrastive Learning of Visual Representations”. In: ICML. 2020.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8LnJlAFB8jfv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of workers: 20\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import logging\n",
        "import sys\n",
        "import zipfile\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import STL10, PCAM\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "## Plot Options\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline.backend_inline\n",
        "import seaborn as sns\n",
        "plt.set_cmap(\"cividis\")\n",
        "%matplotlib inline\n",
        "sns.set_theme()\n",
        "\n",
        "## SKlearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
        "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "print(\"Number of workers:\", NUM_WORKERS)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(666)\n",
        "torch.manual_seed(666)\n",
        "\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "  !pip install gdown==4.6.0 # with the following versions, there is an error\n",
        "  import gdown\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "Hfactory=False # put equal to True, if you are using HFactory\n",
        "if Hfactory:\n",
        "    !pip install gdown==4.6.0 # with the following versions, there is an error  \n",
        "    !pip uninstall -y h5py\n",
        "    !pip install h5py\n",
        "    import h5py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using torch 2.5.1+cu124\n",
            "CUDA device found.\n"
          ]
        }
      ],
      "source": [
        "print(\"Using torch\", torch.__version__)\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print (\"MPS device found.\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\") # we use one GPU, the first one\n",
        "    print (\"CUDA device found.\")\n",
        "else:\n",
        "   device = torch.device(\"cpu\")\n",
        "   print('No MPS or CUDA has been found. PyTorch will use CPU.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbuBZiTN9o12"
      },
      "source": [
        "### Data Augmentation for Contrastive Learning\n",
        "\n",
        "One of the key points of SimCLR is the choice of the augmentation strategy. It composes many different geometric and iconographic transformations.\n",
        "We can implement them very efficiently and easily using the Dataset object of Pytorch.\n",
        "\n",
        "Since in SimCLR authors use 2 views, we do the same here. Please note that we could use more positives\n",
        "\n",
        "The transformations used are: (figure credit - [Ting Chen and Geoffrey Hinton](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/13-contrastive-learning/simclr_data_augmentations.jpg\" width=\"800px\" style=\"padding-top: 10px; padding-bottom: 10px\"></center>\n",
        "\n",
        "When using ImageNet-derived datasets, the two most important transformations are: crop-and-resize, and color distortion.\n",
        "Interestingly, they need to be used together since, when combining randomly cropping and resizing, we might have two situations: (a) cropped image A provides a local view of cropped image B, or (b) cropped images C and D show neighboring views of the same image (figure credit - [Ting Chen and Geoffrey Hinton](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/13-contrastive-learning/crop_views.svg\" width=\"400px\" style=\"padding-top: 20px; padding-bottom: 0px\"></center>\n",
        "\n",
        "While situation (a) requires the model to learn some sort of scale invariance to make crops A and B similar in the representation space, situation (b) is more challenging since the model needs to recognize an object beyond its limited view.\n",
        "However, the network can use the color information (color histograms) to create a useless link between the two patches, without learning generalizable high-level representations. For instance, it could focus on the color of the fur of the dog and on the color of the background to understand that the two patches belong to the same image. That's why, we need to compose crop-and-resize and color distortion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcqluDet_9rZ"
      },
      "source": [
        "**Question**: Create a Pytorch class of transformations (transforms.Compose) where you apply the following augmentations:\n",
        "- Resize an Crop\n",
        "- Horizontal Flip\n",
        "- Color jittering\n",
        "- Gray scale changes\n",
        "- Gaussian Blur\n",
        "\n",
        "Hint: all functions have already been implemented and can be found here: https://pytorch.org/vision/0.9/transforms.html\n",
        "\n",
        "Alternatively, you can also use [albumentations](https://albumentations.ai/) by simply adding:\n",
        "\n",
        "`import albumentations as A`\n",
        "\n",
        "`from albumentations.pytorch.transforms import ToTensorV2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iE89L8tB9xAF"
      },
      "outputs": [],
      "source": [
        "class ContrastiveTransformations:\n",
        "    def __init__(self, img_size, s=1):\n",
        "      # transformations applied in SimCLR article\n",
        "      # \"In this work, we sequentially apply three simple augmentations: random cropping followed by \n",
        "      # resize back to the original size, ran- dom color distortions, and random Gaussian blur\"\n",
        "\n",
        "      self.color_jitter = transforms.ColorJitter(\n",
        "            brightness=0.8 * s, contrast=0.8 * s, saturation=0.8 * s, hue=0.2 * s\n",
        "        )\n",
        "        \n",
        "      self.data_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(size=img_size),\n",
        "            transforms.RandomApply([self.color_jitter], p=0.8),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.GaussianBlur(kernel_size=img_size // 10 * 2 + 1),  \n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization (ImageNet values)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x):\n",
        "      # it outputs a tuple, namely 2 views (augmentations) fo the same image\n",
        "      return self.data_transform(x), self.data_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simple transformation to use networks pre-trained on ImageNet\n",
        "img_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dElKdfNCRyB"
      },
      "source": [
        "**Question**: What's the difference between `ContrastiveTransformations` and `img_transforms` ? Look inside the functions and their outputs..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    #data_directory=\"./\" # decomment it if you do NOT use Google Drive\n",
        "    data_directory=\"/content/drive/MyDrive/TP/\"  # decomment it if you USE Google Drive and copy/paste the path to your Google Drive\n",
        "elif Hfactory:\n",
        "    data_directory=\"./\"    \n",
        "else:\n",
        "    data_directory=\"/users/eleves-a/2024/amine.razig/Representation-Learning-for-CV-and-Medical-Imaging\" # copy/paste the path of the folder with STL dataset in your computer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABAd7xO_qA41"
      },
      "source": [
        "We create unlabeled, training and test Datasets.\n",
        "Please be careful since we use two different transformations, one for the unlabelled part and one for train/test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HXhGEmnT9voP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to /users/eleves-a/2024/amine.razig/Representation-Learning-for-CV-and-Medical-Imagingdata/stl10_binary.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 65.5k/2.64G [00:00<7:05:53, 103kB/s]\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 122] Disk quota exceeded",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m unlabeled_dataset_2viewsCon \u001b[38;5;241m=\u001b[39m STL10(root\u001b[38;5;241m=\u001b[39mdata_directory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munlabeled\u001b[39m\u001b[38;5;124m\"\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mContrastiveTransformations(\u001b[38;5;241m96\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSTL10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_directory\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_transforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# just to show the effect of the augmentations and the classes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_dataset_2viewsCon \u001b[38;5;241m=\u001b[39m STL10(root\u001b[38;5;241m=\u001b[39mdata_directory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mContrastiveTransformations(\u001b[38;5;241m96\u001b[39m))\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/datasets/stl10.py:61\u001b[0m, in \u001b[0;36mSTL10.__init__\u001b[0;34m(self, root, split, folds, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_folds(folds)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/datasets/stl10.py:159\u001b[0m, in \u001b[0;36mSTL10.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity()\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/datasets/utils.py:395\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    393\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 395\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/datasets/utils.py:139\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    137\u001b[0m             _urlretrieve(url, fpath)\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# check integrity of downloaded file\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(fpath, md5):\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/datasets/utils.py:132\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[0;32m--> 132\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/datasets/utils.py:31\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m response\u001b[38;5;241m.\u001b[39mread(chunk_size):\n\u001b[0;32m---> 31\u001b[0m         \u001b[43mfh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
          ]
        }
      ],
      "source": [
        "unlabeled_dataset_2viewsCon = STL10(root=data_directory+\"/data\", split=\"unlabeled\", download=True, transform=ContrastiveTransformations(96))\n",
        "train_dataset = STL10(root=data_directory+\"data\", split=\"train\", download=True, transform=img_transforms)\n",
        "# just to show the effect of the augmentations and the classes\n",
        "train_dataset_2viewsCon = STL10(root=data_directory+\"/data\", split=\"train\", download=True, transform=ContrastiveTransformations(96))\n",
        "test_dataset = STL10(root=data_directory+\"data\", split=\"test\", download=True, transform=img_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMXiU1qX2dqu"
      },
      "outputs": [],
      "source": [
        "# To check the classes in STL10\n",
        "classes=unlabeled_dataset_2viewsCon.classes\n",
        "print(classes)\n",
        "print('Number images in Unlabeled dataset:' ,len(unlabeled_dataset_2viewsCon))\n",
        "print(unlabeled_dataset_2viewsCon[0][0][0].shape) # this is one image (the first of the 2-views tuple)\n",
        "\n",
        "# Train dataset\n",
        "labels=train_dataset.labels # retrieve label of each sample\n",
        "print('Number images in Train dataset:' , len(train_dataset)) # retrieve length of dataset\n",
        "print(train_dataset[3][0].shape) # this is one image\n",
        "\n",
        "#Test dataset\n",
        "print('Number images in Test dataset:' ,len(test_dataset))\n",
        "print(test_dataset[0][0].shape) # this is one image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kioo1xFkQVRa"
      },
      "source": [
        "The Unlabeled dataset contains 100k images. Here, to limit memory requirement, we will use 10% of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbZTvz0RI6j_"
      },
      "outputs": [],
      "source": [
        "sizeUnlabelled=5000\n",
        "\n",
        "unlabeled_dataset_2viewsCon_red,rest = random_split(unlabeled_dataset_2viewsCon, [sizeUnlabelled, len(unlabeled_dataset_2viewsCon)-sizeUnlabelled])\n",
        "len(unlabeled_dataset_2viewsCon_red)\n",
        "del unlabeled_dataset_2viewsCon # free memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOtu79SO_tkw"
      },
      "source": [
        "We can also create a function to visualize the views of each sample, based on the chosen augmentation strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIH9otn52ZR1"
      },
      "outputs": [],
      "source": [
        "def imshowSTL102views(datasetOrig,datasetTransform,rows=5,figsize=(8, 15)):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    for i in range(1, 3*(rows-1)+2,3):\n",
        "      imgOrig = datasetOrig[i][0]\n",
        "      img1=datasetTransform[i][0][0]\n",
        "      img2=datasetTransform[i][0][1]\n",
        "\n",
        "      #REMOVE NORMALIZATION\n",
        "      mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "      std = torch.tensor([0.229, 0.224, 0.225])\n",
        "      unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
        "      # Clip values to range [0,1] -> possible rounding errors during normalization\n",
        "      imgOrig = np.clip(unnormalize(imgOrig).numpy(),0,1)\n",
        "      img1 = np.clip(unnormalize(img1).numpy(),0,1)\n",
        "      img2 = np.clip(unnormalize(img2).numpy(),0,1)\n",
        "\n",
        "      label = datasetOrig[i][1]\n",
        "      fig.add_subplot(rows, 3, i)\n",
        "      plt.title(datasetOrig.classes[label]+ ' , original')\n",
        "      plt.imshow(np.transpose(imgOrig, (1, 2, 0)))\n",
        "      plt.axis(\"off\")\n",
        "      fig.add_subplot(rows, 3, i+1)\n",
        "      plt.title(datasetOrig.classes[label] + ' , 1st view')\n",
        "      plt.imshow(np.transpose(img1, (1, 2, 0)))\n",
        "      plt.axis(\"off\")\n",
        "      fig.add_subplot(rows, 3, i+2)\n",
        "      plt.title(datasetOrig.classes[label] + ' , 2nd view')\n",
        "      plt.imshow(np.transpose(img2, (1, 2, 0)))\n",
        "      plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KmQg476Jqtl"
      },
      "outputs": [],
      "source": [
        "imshowSTL102views(train_dataset,train_dataset_2viewsCon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGh01C9oQvuq"
      },
      "outputs": [],
      "source": [
        "del train_dataset_2viewsCon # To free memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfghymIV9o17"
      },
      "source": [
        "Here, it's the most important part of the code.\n",
        "\n",
        "I remind you that the Siamese architecture of SimCLR is: (figure credit - [Ting Chen et al. ](https://arxiv.org/abs/2006.10029)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/13-contrastive-learning/simclr_network_setup.svg\" width=\"350px\"></center>\n",
        "\n",
        "The employed loss is the InfoNCE loss:\n",
        "$$\n",
        "\\ell_{i,j}=-\\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)}=-\\text{sim}(z_i,z_j)/\\tau+\\log\\left[\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)\\right]\n",
        "$$\n",
        "where $\\tau$ is the temperature and the similarity measure is the cosine similarity:\n",
        "$$\n",
        "\\text{sim}(z_i,z_j) = \\frac{z_i^\\top \\cdot z_j}{||z_i||\\cdot||z_j||}\n",
        "$$\n",
        "The maximum cosine similarity possible is $1$, while the minimum is $-1$.\n",
        "\n",
        "After training, we will remove the projection head $g(\\cdot)$, and use $f(\\cdot)$ as a pretrained feature extractor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wEaYlBvCYPc"
      },
      "source": [
        "**Question**: Comment the code where you see **XXXXXXXXXXXXXXXXXX** explaining in details what the code is doing.\n",
        "\n",
        "PS: if you are using a MacBook, you might have issues with multiprocessing. In that case, please use Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKD0fMKqrOv7"
      },
      "outputs": [],
      "source": [
        "class SimCLR():\n",
        "\n",
        "    def __init__(self, model, optimizer, scheduler, device, batch_size, temperature, epochs):\n",
        "        self.device=device\n",
        "        self.model = model.to(self.device)\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
        "        self.temperature=temperature\n",
        "        self.batch_size=batch_size\n",
        "        self.epochs=epochs\n",
        "\n",
        "\n",
        "    def info_nce_loss(self, features):\n",
        "\n",
        "        # labels for the positives images :\n",
        "        # size of 2*batch_size because two separate data augmentation operators are sampled from the same family of augmentations and applied to each data example to obtain two correlated views. \n",
        "        labels = torch.cat([torch.arange(self.batch_size) for i in range(2)], dim=0)\n",
        "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        features = F.normalize(features, dim=1) # We first normalize the features z_i in the aim to have a unit norm for each feature vector for cosine similarity computation\n",
        "\n",
        "        # computation of cosine similarity between all the features, strored in a matrix\n",
        "        similarity_matrix = torch.matmul(features, features.T)\n",
        "\n",
        "        # We remove the elements of diagonal from the similarity matrix and labels because they correspond to the similarity between the same image\n",
        "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.device)\n",
        "        labels = labels[~mask].view(labels.shape[0], -1)\n",
        "        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
        "\n",
        "        # we recup positive pairs wich are the similarities between transformations of the same image\n",
        "        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n",
        "\n",
        "        # we recup negatives pairs wich are the similarities between transformations of the differents images\n",
        "        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n",
        "\n",
        "        # Concatenatenation of positive and negative pairs to form the logits for computing the loss InfoNCE \n",
        "        logits = torch.cat([positives, negatives], dim=1)\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.device)\n",
        "\n",
        "        # we divide the logits by the temperature because it is a hyperparameter that scales the similarity between the features\n",
        "        logits = logits / self.temperature\n",
        "        return logits, labels\n",
        "\n",
        "    def train(self, train_loader, use_amp = False):\n",
        "    \n",
        "        # Initialize the GradScaler if using mixed-precision training\n",
        "        scaler = GradScaler(enabled=use_amp) if torch.cuda.is_available() else None\n",
        "\n",
        "        n_iter = 0\n",
        "        print(\"Start SimCLR training for {} epochs.\".format(self.epochs))\n",
        "\n",
        "        for epoch_counter in range(self.epochs):\n",
        "            for images, _ in tqdm(train_loader):\n",
        "                \n",
        "                # We concatenate the two augmented views generated by t and t' of the images into a single batch\n",
        "                images = torch.cat(images, dim=0)\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if torch.cuda.is_available() and use_amp:\n",
        "                  with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp): # to improve performance while maintaining accuracy. \n",
        "                    features = self.model(images)\n",
        "                    logits, labels = self.info_nce_loss(features)\n",
        "                    loss = self.criterion(logits, labels)\n",
        "                else:   #autocast and mixed-precision training with FP16 is not suited for CPU                           \n",
        "                  features = self.model(images)\n",
        "                  logits, labels = self.info_nce_loss(features)\n",
        "                  # We compute the loss InfoNCE using cross-entropy criterion \n",
        "                  loss = self.criterion(logits, labels)     \n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                if scaler is not None:\n",
        "                  scaler.scale(loss).backward()\n",
        "                  scaler.step(self.optimizer)\n",
        "                  scaler.update()\n",
        "                else:\n",
        "                  loss.backward()\n",
        "                  self.optimizer.step()\n",
        "\n",
        "                n_iter += 1\n",
        "\n",
        "            # warmup for the first 10 epochs\n",
        "            if epoch_counter >= 5:\n",
        "                self.scheduler.step()\n",
        "\n",
        "            print('Epoch: {}, Average loss: {:.4f}, lr: {:.4f}'.format(epoch_counter, loss / len(train_loader.dataset), self.scheduler.get_last_lr()[0] ))\n",
        "\n",
        "        print(\"Training has finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBzpMRY_9o18"
      },
      "source": [
        "As before, we use a DataLoader.\n",
        "\n",
        "DataLoader wraps an iterable around the Dataset to enable easy access to the samples. The Dataset retrieves our dataset features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python multiprocessing to speed up data retrieval. DataLoader is an iterable that abstracts this complexity for us in an easy API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIfB8usLnw_H"
      },
      "outputs": [],
      "source": [
        "bs = XXXXXXX # choose an appropriate batch size depending on the computational resources\n",
        "\n",
        "train_unlabelled_loader = DataLoader(dataset=unlabeled_dataset_2viewsCon_red, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True, persistent_workers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZzHMeG09o19"
      },
      "source": [
        "Here we define the hyper-parameters, optimization, scheduler and launch the training.\n",
        "\n",
        "**Question**: complete the code where you see XXXXX.\n",
        "\n",
        "As model for $f()$, use a ResnNet18 not-pretrained. Remember that the model ResNet18 has already a Linear Layer at the end (fc) which can be written as $Wf(x)$. You can also change the `out_features` of the fc by adding the option `num_classes=XXXX` while loading the model.\n",
        "\n",
        " As projection head, use the one from the article\n",
        "\n",
        " $$ g(f(x))=W^a \\sigma (W^b f(x))$$\n",
        "\n",
        " where $\\sigma$ is a Relu non-linearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_epochs=2\n",
        "lr=0.003\n",
        "wd=1e-4\n",
        "temperature=0.07\n",
        "#f_dim=512 # to use if you want to change the output dimension of f\n",
        "g_dim=128 # the output dimension of the projection head\n",
        "\n",
        "# Ensure that you are using GPU and all CPU workers\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Encoder f\n",
        "f = XXXXXXXXX #  Wf()\n",
        "\n",
        "# Projection head g()\n",
        "g = XXXXXX\n",
        "f.fc= XXXXXXXX\n",
        "\n",
        "optimizer = torch.optim.Adam(f.parameters(), lr=lr, weight_decay=wd)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_unlabelled_loader), eta_min=0, last_epoch=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iuDyoz2vmbL"
      },
      "outputs": [],
      "source": [
        "simclr = SimCLR(model=f, optimizer=optimizer, scheduler=scheduler, device=device, batch_size=bs, temperature=temperature, epochs=max_epochs)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    simclr.train(train_unlabelled_loader, use_amp = True)\n",
        "else:\n",
        "    simclr.train(train_unlabelled_loader, use_amp = False)\n",
        "\n",
        "# save model checkpoints\n",
        "os.makedirs('models/', exist_ok=True)\n",
        "filename = 'models/resnet18_simclr_2epochs_stl10.pth.tar'\n",
        "torch.save({\n",
        "                'epoch': max_epochs,\n",
        "                'state_dict': simclr.model.state_dict()\n",
        "            }, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je6wLYpA9o19"
      },
      "source": [
        "To continue, you can use a model that I have already pre-trained for 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FER2FZWlIKJd"
      },
      "outputs": [],
      "source": [
        "model_path = 'models/resnet18_simclr_100epochs_stl10.pth.tar'\n",
        "\n",
        "if os.path.isfile(data_directory+model_path):\n",
        "    print(\"pre-trained model already downloaded\")\n",
        "else:\n",
        "    print(\"downloading the pre-trained model\")\n",
        "    os.makedirs(data_directory+'models/', exist_ok=True)\n",
        "    file_url = 'https://drive.google.com/uc?id=13_ZueA9mqh17GvYVkfU_Yokg3z065rKG'    \n",
        "    gdown.download(file_url, data_directory+model_path)\n",
        "\n",
        "\n",
        "# Load checkpoint file of already trained model\n",
        "checkpoint = torch.load(data_directory+model_path, map_location=torch.device(device))\n",
        "\n",
        "# Load Model parameters and set it into eval mode\n",
        "model_download = models.resnet18(weights=None) # we will only use f and not g\n",
        "model_download.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "\n",
        "epoch = checkpoint['epoch']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t9itgta9o19"
      },
      "source": [
        "As previously explained, we can now use $f()$ to encode the samples discarding $g()$.\n",
        "\n",
        "**Question** How can you discard the projection ? Complete the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGMQg40HWGI8"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def prepare_data_features(model, dataset, batchsize,  device):\n",
        "    # Prepare model\n",
        "    network = deepcopy(model)\n",
        "\n",
        "    # you need to modify network here\n",
        "    XXXXXXXXX\n",
        "    \n",
        "    network.eval()\n",
        "    network.to(device)\n",
        "\n",
        "    # Encode all images\n",
        "    data_loader = data.DataLoader(dataset, batch_size=batchsize, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
        "\n",
        "    feats, labels = [], []\n",
        "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
        "        batch_imgs = batch_imgs.to(device)\n",
        "        batch_feats = network(batch_imgs)\n",
        "        feats.append(batch_feats.detach().cpu())\n",
        "        labels.append(batch_labels)\n",
        "\n",
        "    feats = torch.cat(feats, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "    # Sort images by labels\n",
        "    labels, idxs = labels.sort()\n",
        "    feats = feats[idxs]\n",
        "\n",
        "    return data.TensorDataset(feats, labels), [feats.numpy() , labels.numpy()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pc8-XK49o1-"
      },
      "source": [
        "We can use either the trained model or the downloaded model and encode the train and test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdYnp20mVBPu"
      },
      "outputs": [],
      "source": [
        "#modelTrained=simclr.model\n",
        "modelTrained=model_download\n",
        "trainloader, [train_feats, train_labels] = prepare_data_features(modelTrained, train_dataset, batchsize=256, device=device)\n",
        "testloader, [test_feats, test_labels] = prepare_data_features(modelTrained, test_dataset, batchsize=256, device=device)\n",
        "print(train_feats.shape, train_labels.shape)\n",
        "print(test_feats.shape, test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McW8aTD09o1-"
      },
      "source": [
        "**Linear Probe**:  we train a logistic regression on the train dataset and evaluate it on the test dataset. This is called Linear Probe.\n",
        "\n",
        "**Question**: compute the training and test errore using a logistic regression where you are free to use a regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5TR-EFodecU"
      },
      "outputs": [],
      "source": [
        "# compute linear probe results\n",
        "XXXXXXXX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he8wW24A9o1-"
      },
      "source": [
        "What if we simply used it a pre-trained model on ImageNet ? ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vkM1zV3L9SF"
      },
      "source": [
        "**Question**: use the previous function `prepare_data_features` with a resnet18 pre-trained on Imagenet and compute the Linear Probe as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jhl8MY89o1_"
      },
      "outputs": [],
      "source": [
        "modelImageNet = XXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P3-Uhri9o1_"
      },
      "source": [
        "**Question**: is the result better ? Why in your opinion ? What could you do to improve the method with the worst result ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HBhhuB09o1_"
      },
      "source": [
        "To go further, you could use one of the datasets present in Med Mnist (https://medmnist.com/). These are real but rescaled images. Small images allow the training of DL models on small GPU but we loose resolution and thus clinically relevant information. \n",
        "\n",
        "Otherwise, you could use the PatchCamelyon dataset (https://www.kaggle.com/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon), which comprises real and large images.\n",
        "\n",
        "The PatchCamelyon benchmark is a image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annoted with a binary label indicating presence of metastatic tissue.\n",
        "\n",
        "You can donwload it from pytorch vision or, if you are using Google Colab, directly from our Google drive. The pytorch version needs to be unzipped and there is not enough RAM memory in the Google Colab servers...\n",
        "\n",
        "Please do not change the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0779Lfnm9o1_"
      },
      "outputs": [],
      "source": [
        "os.makedirs(data_directory+\"data/pcam\", exist_ok=True)\n",
        "\n",
        "# Download the Train set\n",
        "file_url = 'https://drive.google.com/uc?id=1ipIG12YWag54v2_2JIyfPiZDN0Eu3IjB'\n",
        "train_path = 'camelyonpatch_level_2_split_train_x.h5'\n",
        "if os.path.isfile(data_directory+\"data/pcam/\"+train_path):\n",
        "    print(\"training set already downloaded\")\n",
        "else:\n",
        "    print(\"downloading training set\")\n",
        "    gdown.download(file_url, data_directory+\"data/pcam/\"+train_path, quiet=False)\n",
        "\n",
        "# Download Valid set\n",
        "file_url = 'https://drive.google.com/uc?id=1emdhTV8J8Pv-SjKSoMzE_SbT04Ik2yUm'\n",
        "valid_path = 'camelyonpatch_level_2_split_valid_x.h5'\n",
        "if os.path.isfile(data_directory+\"data/pcam/\"+valid_path):\n",
        "    print(\"validation set already downloaded\")\n",
        "else:\n",
        "    print(\"downloading validation set\")\n",
        "    gdown.download(file_url, data_directory+\"data/pcam/\"+valid_path, quiet=False)\n",
        "\n",
        "# Download Test set\n",
        "file_url = 'https://drive.google.com/uc?id=1dkeFapKSKm-wUtf9zicxiSHWIS0uxv8Z'\n",
        "test_path = 'camelyonpatch_level_2_split_test_x.h5'\n",
        "if os.path.isfile(data_directory+\"data/pcam/\"+test_path):\n",
        "    print(\"test set already downloaded\")\n",
        "else:\n",
        "    print(\"downloading test set\")\n",
        "    gdown.download(file_url, data_directory+\"data/pcam/\"+test_path, quiet=False)\n",
        "\n",
        "# Download the labels\n",
        "file_url = 'https://drive.google.com/uc?id=10ftBj2ZiiDESTsANdF-v8oh4NieinYPP'\n",
        "labels_path = 'Label.zip'\n",
        "if os.path.isfile(data_directory+\"data/pcam/camelyonpatch_level_2_split_test_y.h5\"):\n",
        "    print(\"Labels already downloaded\")\n",
        "else:\n",
        "    print(\"downloading labels\")\n",
        "    gdown.download(file_url, data_directory+\"data/pcam/\"+labels_path, quiet=False)\n",
        "    # Unzip and move to data directory\n",
        "    with zipfile.ZipFile(data_directory+\"data/pcam/\"+labels_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_directory+\"data/pcam/\")\n",
        "    # Moving files\n",
        "    os.rename(data_directory+\"data/pcam/Labels/camelyonpatch_level_2_split_test_y.h5\", data_directory+\"data/pcam/camelyonpatch_level_2_split_test_y.h5\")\n",
        "    os.rename(data_directory+\"data/pcam/Labels/camelyonpatch_level_2_split_train_y.h5\", data_directory+\"data/pcam/camelyonpatch_level_2_split_train_y.h5\")\n",
        "    os.rename(data_directory+\"data/pcam/Labels/camelyonpatch_level_2_split_valid_y.h5\", data_directory+\"data/pcam/camelyonpatch_level_2_split_valid_y.h5\")\n",
        "    # Cleaning\n",
        "    os.remove(data_directory+\"data/pcam/\"+labels_path)\n",
        "    os.rmdir(data_directory+\"data/pcam/Labels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-968M16HGUr"
      },
      "source": [
        "Now you can load it using the PCAM Dataset and DataLoader (keep download as False since you have already downloaded the data)\n",
        "\n",
        "In the following, you will use the training split as if it was unlabeled, thus for the self-supervised part, then the val split as if it was the training set and the test split as test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oV0QM3pOjqA"
      },
      "outputs": [],
      "source": [
        "img_size=96\n",
        "bs = 128\n",
        "\n",
        "# ### PCAM Dataset\n",
        "\n",
        "PCAM_unlabeled_dataset = PCAM(root=data_directory+\"data\", download=False, split='train', transform=ContrastiveTransformations(img_size))\n",
        "PCAM_train_dataset = PCAM(root=data_directory+\"data\", download=False, split='val', transform=transforms.ToTensor())\n",
        "PCAM_test_dataset = PCAM(root=data_directory+\"data\", download=False, split='test', transform=transforms.ToTensor())\n",
        "print('There are: ', len(PCAM_unlabeled_dataset), 'training images; ', len(PCAM_train_dataset), ' validation images; and ', len(PCAM_test_dataset), ' test images')\n",
        "\n",
        "\n",
        "PCAM_unlabeled_loader = DataLoader(dataset=PCAM_unlabeled_dataset, batch_size=bs, num_workers=NUM_WORKERS, shuffle=True)\n",
        "PCAM_train_loader = DataLoader(PCAM_train_dataset, batch_size=bs)\n",
        "PCAM_test_loader = DataLoader(PCAM_test_dataset, batch_size=bs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LC-5u1xHr3Q"
      },
      "source": [
        "We can visualize the images as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiaxlpr-PfjU"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "NUM_IMAGES = 12\n",
        "PCAM_images = torch.stack([PCAM_train_dataset[np.random.randint(len(PCAM_train_dataset))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(PCAM_images, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the PCAM dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3oak1EQJMAQ"
      },
      "source": [
        "**Question**: As before, train the SimCLR algorithm on the unlabeled set and then train and test the Linear Probe.\n",
        "\n",
        "This time, you can use the ResNet-18 pre-trained on ImageNet and train it for just 10 epochs.\n",
        "If you want to speed up the computations (a bit) you can also reduce the size of the images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt6UjBRbQv-m"
      },
      "outputs": [],
      "source": [
        "max_epochs=20\n",
        "lr=0.003\n",
        "wd=1e-4\n",
        "temperature=0.07\n",
        "\n",
        "\n",
        "model = models.resnet18(weights=None, num_classes=128)\n",
        "dim_mlp = model.fc.in_features\n",
        "model.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), model.fc)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(PCAM_unlabeled_loader), eta_min=0, last_epoch=-1)\n",
        "\n",
        "simclr = SimCLR(model=model, optimizer=optimizer, scheduler=scheduler, device=device, batch_size=bs, temperature=temperature, epochs=max_epochs)\n",
        "simclr.train(PCAM_unlabeled_loader)\n",
        "\n",
        "# save model checkpoints\n",
        "os.makedirs('models/', exist_ok=True)\n",
        "filename = 'models/resnet18_simclr_100epochs_PCAM.pth.tar'\n",
        "torch.save({\n",
        "                'epoch': max_epochs,\n",
        "                'state_dict': simclr.model.state_dict()\n",
        "            }, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-pacHnZtAar"
      },
      "source": [
        "**Question**: Using the linear probe, test whether the self-supervised training (for 10 or more epochs) improves the results over a simple ResNet18 only pre-trained on ImageNet. What would you do if you had more time ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "469_Y50HJG13"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
