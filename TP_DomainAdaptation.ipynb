{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omJPbIm-BVf8"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import numpy as np\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import SVHN, MNIST, USPS\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "## Sklearn\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "## UMAP\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "  print(\"Using Google Colab\")\n",
        "  !pip install umap-learn\n",
        "  from umap import UMAP\n",
        "  !pip install adapt\n",
        "  !pip install Tensorflow==2.15\n",
        "else:\n",
        "  from umap import UMAP  \n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded\n",
        "DATASET_PATH = \"./data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using torch 2.5.1\n",
            "MPS (MacBook) device found.\n",
            "Number of MPS cards:  1\n",
            "Total MPS memory 28.99 GB\n"
          ]
        }
      ],
      "source": [
        "print(\"Using torch\", torch.__version__)\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print (\"MPS (MacBook) device found.\")\n",
        "    #print('Number of MPS cards: ', torch.mps.device_count())\n",
        "    #print('Total MPS memory {0:.2f} GB'.format(torch.mps.recommended_max_memory()/pow(10,9)))\n",
        "elif torch.backends.cuda.is_built():\n",
        "    device = torch.device(\"cuda\")\n",
        "    # Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print (\"CUDA device found.\")\n",
        "    print('Number of GPU cards: ', torch.cuda.device_count(), '\\nWhich card GPU?', torch.cuda.get_device_name(0))\n",
        "    print('Total GPU memory {1:.2f} GB. Free GPU memory {0:.2f} GB'.format(torch.cuda.mem_get_info()[0]/pow(10,9),torch.cuda.mem_get_info()[1]/pow(10,9)))\n",
        "else:\n",
        "   device = torch.device(\"cpu\")\n",
        "   print('No MPS or CUDA has been found. PyTorch will use CPU.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSQ7egRS4Zy6"
      },
      "source": [
        "In this practical session you will implement and test the method presented in [1] for Unsupervised Domain Adaptation.\n",
        "\n",
        "You will use three imaging datasets showing 0-9 digits: MNIST, SVHN and USPS.\n",
        "\n",
        "We know that the output spaces are equal (same labels/classes), namely the digits from 0 to 9, and we will resize the data so that the input spaces are also equal (same number of pixels per image).\n",
        "\n",
        "In this TP, you will assume a covariate shift and therefore that there is a shift between the marginal disitributions $p_S(X)$ and $p_T(X)$ and that the conditionl distributions $p_S(Y|X)$ and $p_t(Y|X)$ are equal.\n",
        "\n",
        "As for the other TP, answer all questions and fill the code where you see **XXXXXXXXX**\n",
        "\n",
        "**Deadline**: please verify on the Moodle/Ecampus the deadline\n",
        "\n",
        "\n",
        "\n",
        "[1] B. Fernando et al. “Unsupervised Visual Domain Adaptation Using Subspace Alignment”. In: ICCV. 2013."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Lk1d8B6OHj"
      },
      "source": [
        "### **MNIST**\n",
        "\n",
        " Let's start by downloading the **MNIST** dataset: a very common and large database of grayscale images showing handwritten digits ranging from 0 to 9. It comprises 60,000 training images and 10,000 testing images of size 28x28.\n",
        "\n",
        "To cope with the assumption about the \"same number of input pixels per image\", we choose the input dimension of 32x32 for all datasets. We thus resize all images (originally 28x28) using the padding function \"Pad\" (add 0 to the borders).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmoV4d9UCP6X"
      },
      "outputs": [],
      "source": [
        "# Transform to normalized Tensors in the range [0,1]\n",
        "MNIST_train_dataset = MNIST(root=DATASET_PATH, train= True, download=True, transform=T.Compose([T.Pad(2), T.ToTensor()]))\n",
        "MNIST_test_dataset = MNIST(root=DATASET_PATH, train= False, download=True, transform=T.Compose([T.Pad(2), T.ToTensor()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3ikBd5M1_BS"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "NUM_IMAGES = 12\n",
        "MNIST_images = torch.stack([MNIST_train_dataset[np.random.randint(len(MNIST_train_dataset))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(MNIST_images, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the MNIST dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s12W41ocL-Xf"
      },
      "source": [
        "Be careful ! Even if you put a transform when downloading the data and creating teh Pytorch Dataset, this is actually not applied until you load the data with a DataLoader.\n",
        "\n",
        "For instance, when you load images in the range $[0,255]$ and you put the transform `ToTensor()`, this should automatically normalize the images in the range $[0,1]$ and transform them into Pytorch tensors. However, if you simply take the data from the DataSet *WITHOUT* the DataLoader..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72Nwyk0VL9oA"
      },
      "outputs": [],
      "source": [
        "#This does not apply the normalization to [0-1] (transform To.Tensor() )\n",
        "MNIST_Xtrain_255 = MNIST_train_dataset.data.numpy()\n",
        "MNIST_ytrain_255 = MNIST_train_dataset.targets.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1XCNgaeNA71"
      },
      "outputs": [],
      "source": [
        "print(MNIST_Xtrain_255.shape)\n",
        "print(MNIST_ytrain_255.shape)\n",
        "\n",
        "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(MNIST_Xtrain_255.min(), MNIST_Xtrain_255.max()))\n",
        "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(MNIST_Xtrain_255.mean(), MNIST_Xtrain_255.std()))\n",
        "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(MNIST_Xtrain_255.min(), MNIST_Xtrain_255.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhckfU-SNM89"
      },
      "source": [
        "Instead, if we load them with the Dataloader..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZkyDwAVfx-U"
      },
      "outputs": [],
      "source": [
        "MNIST_train_loader = DataLoader(MNIST_train_dataset, batch_size=len(MNIST_train_dataset))\n",
        "MNIST_test_loader = DataLoader(MNIST_test_dataset, batch_size=len(MNIST_test_dataset))\n",
        "\n",
        "MNIST_Xtrain_ima = np.squeeze(next(iter(MNIST_train_loader))[0].numpy())\n",
        "MNIST_ytrain = next(iter(MNIST_train_loader))[1].numpy()\n",
        "\n",
        "MNIST_Xtest_ima = np.squeeze(next(iter(MNIST_test_loader))[0].numpy())\n",
        "MNIST_ytest = next(iter(MNIST_test_loader))[1].numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glLJoyhRa2ux"
      },
      "outputs": [],
      "source": [
        "print(MNIST_Xtrain_ima.shape)\n",
        "print(MNIST_ytrain.shape)\n",
        "\n",
        "print(MNIST_Xtest_ima.shape)\n",
        "print(MNIST_ytest.shape)\n",
        "\n",
        "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(MNIST_Xtrain_ima.min(), MNIST_Xtrain_ima.max()))\n",
        "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(MNIST_Xtrain_ima.mean(), MNIST_Xtrain_ima.std()))\n",
        "\n",
        "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(MNIST_Xtest_ima.min(), MNIST_Xtest_ima.max()))\n",
        "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(MNIST_Xtest_ima.mean(), MNIST_Xtest_ima.std()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1ldED41Sjw6"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "NUM_IMAGES=12\n",
        "MNIST_grid = MNIST_Xtrain_ima[np.random.randint(0,len(MNIST_Xtrain_ima),12),:,:]\n",
        "print(MNIST_grid.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                 nrows_ncols=(2, 6),  # creates 2x2 grid of axes\n",
        "                 axes_pad=0.1,  # pad between axes in inch.\n",
        "                 )\n",
        "\n",
        "for ax, im in zip(grid, MNIST_grid):\n",
        "    # Iterating over the grid returns the Axes.\n",
        "    ax.imshow(im,cmap='gray')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iKxBzhzPveU"
      },
      "source": [
        "So we have 60k training images and 10K test images of size $[32 x 32]$. Let's load another dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9WKoUNfBcAd"
      },
      "source": [
        "### **SVHN dataset**\n",
        "\n",
        "The Street View House Numbers (SVHN) dataset is a real-world image dataset for house number detection. It is similar to MNIST since it has the same digit classes (0 to 9), but it is significantly harder due to its real-world setting. SVHN is obtained from house numbers in Google Street View images.\n",
        "\n",
        "It comprises RGB images of size $32x32$\n",
        "\n",
        "We change them from RGB to grayscale to compare them with the MNIST images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsZpWGRSBdfL"
      },
      "outputs": [],
      "source": [
        "SVHN_train_dataset = SVHN(root=DATASET_PATH, split='train', download=True, transform=T.Compose([T.Grayscale(), T.ToTensor()]))\n",
        "SVHN_test_dataset = SVHN(root=DATASET_PATH, split='test', download=True, transform=T.Compose([T.Grayscale(), T.ToTensor()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_1RIyZyBfkq"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "NUM_IMAGES = 12\n",
        "SVHN_images = torch.stack([SVHN_test_dataset[np.random.randint(len(SVHN_test_dataset))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(SVHN_images, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the SVHN dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgfUnfz1QhVX"
      },
      "outputs": [],
      "source": [
        "SVHN_train_loader = DataLoader(SVHN_train_dataset, batch_size=len(SVHN_train_dataset))\n",
        "SVHN_test_loader = DataLoader(SVHN_test_dataset, batch_size=len(SVHN_test_dataset))\n",
        "\n",
        "SVHN_Xtrain_ima = np.squeeze(next(iter(SVHN_train_loader))[0].numpy())\n",
        "SVHN_ytrain = next(iter(SVHN_train_loader))[1].numpy()\n",
        "\n",
        "SVHN_Xtest_ima = np.squeeze(next(iter(SVHN_test_loader))[0].numpy())\n",
        "SVHN_ytest = next(iter(SVHN_test_loader))[1].numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SKHMw74Q-Jv"
      },
      "outputs": [],
      "source": [
        "print(SVHN_Xtrain_ima.shape)\n",
        "print(SVHN_ytrain.shape)\n",
        "print(SVHN_Xtest_ima.shape)\n",
        "print(SVHN_ytest.shape)\n",
        "\n",
        "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(SVHN_Xtrain_ima.min(), SVHN_Xtrain_ima.max()))\n",
        "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(SVHN_Xtrain_ima.mean(), SVHN_Xtrain_ima.std()))\n",
        "\n",
        "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(SVHN_Xtest_ima.min(), SVHN_Xtest_ima.max()))\n",
        "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(SVHN_Xtest_ima.mean(), SVHN_Xtest_ima.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkGBoOLEVpjW"
      },
      "source": [
        "Here we have ~73K training images and ~26K test images of the same size $[32 x x32]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV8g71Qv8glj"
      },
      "source": [
        "### **USPS**\n",
        "\n",
        "As last dataset, we will use the USPS dataset which contains images of digits (from 0 to 9) scanned from envelopes by the U.S. Postal Service. It contains a total of 9,298 $16x16$ pixel grayscale images. Images are centered, normalized and show a broad range of font styles.\n",
        "\n",
        "Even in ths case, we pad them with 0 in order to have $32 x 32$ images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84hLNkXXaDrL"
      },
      "outputs": [],
      "source": [
        "USPS_train_dataset = USPS(root=DATASET_PATH, train= True, download=True, transform=T.Compose([T.Pad(8), T.ToTensor()]))\n",
        "USPS_test_dataset = USPS(root=DATASET_PATH, train= False, download=True, transform=T.Compose([T.Pad(8), T.ToTensor()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DffRm-JYaRMZ"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "NUM_IMAGES = 12\n",
        "USPS_images = torch.stack([USPS_train_dataset[np.random.randint(len(USPS_train_dataset))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(USPS_images, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the USPS dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9VzP5qAWmpD"
      },
      "outputs": [],
      "source": [
        "USPS_train_loader = DataLoader(USPS_train_dataset, batch_size=len(USPS_train_dataset))\n",
        "USPS_test_loader = DataLoader(USPS_test_dataset, batch_size=len(USPS_test_dataset))\n",
        "\n",
        "USPS_Xtrain_ima = np.squeeze(next(iter(USPS_train_loader))[0].numpy())\n",
        "USPS_ytrain = next(iter(USPS_train_loader))[1].numpy()\n",
        "\n",
        "USPS_Xtest_ima = np.squeeze(next(iter(USPS_test_loader))[0].numpy())\n",
        "USPS_ytest = next(iter(USPS_test_loader))[1].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpgXJtNuW2gx"
      },
      "outputs": [],
      "source": [
        "print(USPS_Xtrain_ima.shape)\n",
        "print(USPS_ytrain.shape)\n",
        "print(USPS_Xtest_ima.shape)\n",
        "print(USPS_ytest.shape)\n",
        "\n",
        "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(USPS_Xtrain_ima.min(), USPS_Xtrain_ima.max()))\n",
        "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(USPS_Xtrain_ima.mean(), USPS_Xtrain_ima.std()))\n",
        "\n",
        "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(USPS_Xtest_ima.min(), USPS_Xtest_ima.max()))\n",
        "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(USPS_Xtest_ima.mean(), USPS_Xtest_ima.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbUbRIUIXgUq"
      },
      "source": [
        "We have ~7K training images and ~2k test images of size $[32 x 32]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ShELte9_b-y"
      },
      "source": [
        "### **UMAP**\n",
        "\n",
        "One of the first things to do, is to visually check the data to verify if it exists a domain gap and if the datasets are different.\n",
        "\n",
        "Here, we will use the UMAP and t-SNE methods which are general manifold learning and non-linear dimension reduction algorithms (probably the two most-used ones). UMAP can be seen as a Riemannian extension of the t-SNE method (which uses an Euclidean distance).\n",
        "\n",
        "[1] Leland McInnes, John Healy, James Melville. \"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\". 2018\n",
        "\n",
        "[2] van der Maaten, L.J.P., Hinton, G.E. \"Visualizing Data Using t-SNE\". Journal of Machine Learning Research. 2008\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpIanfPGUPJn"
      },
      "source": [
        "Transform data so that instead than having 3D arrays of images we will have 2D arrays by vectorizing each image -> 32 x 32 = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ye0wpUeb0Wm"
      },
      "outputs": [],
      "source": [
        "# MNIST\n",
        "MNIST_Xtrain=XXXXX(MNIST_Xtrain_ima,XXXXXX)\n",
        "MNIST_Xtest=XXXXXX(MNIST_Xtest_ima,XXXXXX)\n",
        "\n",
        "print(MNIST_Xtrain.shape, MNIST_Xtest.shape)\n",
        "\n",
        "# USPS\n",
        "USPS_Xtrain=XXXXXX(USPS_Xtrain_ima,XXXXXX)\n",
        "USPS_Xtest=XXXXXX(USPS_Xtest_ima,XXXXXX)\n",
        "\n",
        "print(USPS_Xtrain.shape, USPS_Xtest.shape)\n",
        "\n",
        "# SVHN\n",
        "SVHN_Xtrain=XXXXXX(SVHN_Xtrain_ima,XXXXXX)\n",
        "SVHN_Xtest=XXXXXX(SVHN_Xtest_ima,XXXXXX)\n",
        "\n",
        "print(SVHN_Xtrain.shape, SVHN_Xtest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXz-SHpbW1x6"
      },
      "source": [
        "Here, we randomly pick only 1000 samples per dataset for visualization and to reduce the computational burden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA21XminThg5"
      },
      "outputs": [],
      "source": [
        "# Reduced MNIST\n",
        "XXXXXX\n",
        "MNIST_Xtrain_res = XXXXXX\n",
        "MNIST_ytrain_res = XXXXXX\n",
        "\n",
        "# Reduced USPS\n",
        "XXXXXX\n",
        "USPS_Xtrain_res = XXXXXX\n",
        "USPS_ytrain_res = XXXXXX\n",
        "\n",
        "# Reduced SVHN\n",
        "XXXXXX\n",
        "SVHN_Xtrain_res = XXXXXX\n",
        "SVHN_ytrain_res = XXXXXX\n",
        "\n",
        "\n",
        "# Reduced Data\n",
        "xtot=np.concatenate((MNIST_Xtrain_res,USPS_Xtrain_res,SVHN_Xtrain_res),axis=0) # all data\n",
        "nMNIST=MNIST_Xtrain_res.shape[0]\n",
        "nUSPS=USPS_Xtrain_res.shape[0]\n",
        "nSVHN=SVHN_Xtrain_res.shape[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvvoJpSFa3R4"
      },
      "outputs": [],
      "source": [
        "xp = UMAP().fit_transform(xtot) # this should take ~50sec\n",
        "\n",
        "print(xp.shape)\n",
        "\n",
        "# separate again but now in 2D\n",
        "xpMNIST=xp[:nMNIST,:]\n",
        "xpUSPS=xp[nMNIST:nMNIST+nUSPS,:]\n",
        "xpSVHN=xp[nMNIST+nUSPS:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDi0W-QTfR2a"
      },
      "outputs": [],
      "source": [
        "plt.figure(3,(12,10))\n",
        "\n",
        "plt.scatter(xpMNIST[:,0],xpMNIST[:,1],c=MNIST_ytrain_res,marker='o',cmap='tab10',label='MNIST')\n",
        "plt.scatter(xpUSPS[:,0],xpUSPS[:,1],c=USPS_ytrain_res,marker='x',cmap='tab10',label='USPS')\n",
        "plt.scatter(xpSVHN[:,0],xpSVHN[:,1],c=SVHN_ytrain_res,marker='s',cmap='tab10',label='SVHN')\n",
        "plt.legend()\n",
        "plt.colorbar()\n",
        "plt.title('UMAP Embedding of the data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghpupFXiBwtp"
      },
      "source": [
        "It seems that one dataset is quite different from the other two.\n",
        "\n",
        "**Question**: What are your conclusions ? Re-run the code showing only the most similar datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBcQQXzqiXl7"
      },
      "outputs": [],
      "source": [
        "XXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GHPuni7B-dY"
      },
      "source": [
        "**Question**: Can you see a domain shift between the datasets ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th7gqZFDCD74"
      },
      "source": [
        "Now we can test the t-SNE algorithm to check whether your conclusions are confirmed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8Cnj8_bW_p5"
      },
      "outputs": [],
      "source": [
        "%time xp=TSNE().fit_transform(xtot) # this should take around 50 sec\n",
        "\n",
        "print(xp.shape)\n",
        "\n",
        "# separate again but now in 2D\n",
        "xpMNIST=xp[:nMNIST,:]\n",
        "xpUSPS=xp[nMNIST:nMNIST+nUSPS,:]\n",
        "xpSVHN=xp[nMNIST+nUSPS:,:]\n",
        "print(xpMNIST.shape,xpUSPS.shape,xpSVHN.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trzKdwXhY1cD"
      },
      "outputs": [],
      "source": [
        "plt.figure(3,(12,10))\n",
        "\n",
        "plt.scatter(xpMNIST[:,0],xpMNIST[:,1],c=MNIST_ytrain_res,marker='o',cmap='tab10',label='MNIST')\n",
        "plt.scatter(xpUSPS[:,0],xpUSPS[:,1],c=USPS_ytrain_res,marker='x',cmap='tab10',label='USPS')\n",
        "plt.scatter(xpSVHN[:,0],xpSVHN[:,1],c=SVHN_ytrain_res,marker='s',cmap='tab10',label='SVHN')\n",
        "plt.legend()\n",
        "plt.colorbar()\n",
        "plt.title('TSNE Embedding of the data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip2L9qjCPNBW"
      },
      "source": [
        "**Question**: can you confirm your previous conclusions ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_djO75CmPlTY"
      },
      "source": [
        "It seems that one dataset is quite different from the other two data-sets.\n",
        "\n",
        "**Question**: Analyse that dataset alone using UMAP (faster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yny7ogrOi5Ch"
      },
      "outputs": [],
      "source": [
        "XXXXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAbRzv571BOf"
      },
      "source": [
        "**Question**: Are the data clearly clustered and well separated as for the other two datasets ? If not, why is that in your opinion ?\n",
        "\n",
        "Hint: think about the input space of UMAP..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83AeEk8t_-mI"
      },
      "source": [
        "# Unsupervised Domain Adaptation\n",
        "\n",
        "The three data-sets have the same features $\\mathcal{X}$ (all images have size $[32 x 32]$)\n",
        " and the same classes (output labels) $\\mathcal{Y}$ but they have different marginal $p(X)$ distributions.\n",
        "\n",
        "In this part, we will assume that we only have labeled data in the source domain $S$ but not int the target domain $T$. By matching the source $p_S(X)$ and target $p_T(X)$ marginal distributions, one can hope that a classifier learnt on $S$ will also work on $T$.\n",
        "\n",
        "Here, you will implement and test the following algorithm based on subspace alignement:\n",
        "\n",
        "B. Fernando et al. \"*Unsupervised Visual Domain Adaptation Using Subspace Alignment*\". ICCV, 2013."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB34ClfdFhC-"
      },
      "source": [
        "The first thing to do is to choose the source domain and the target domain.\n",
        "Let's start with the MNIST dataset as source and USPS as target.\n",
        "\n",
        "We will use the previousyly computed reduced datasets to speed up computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBEp77urFlqX"
      },
      "outputs": [],
      "source": [
        "# Source domain\n",
        "XtrainS = MNIST_Xtrain_res\n",
        "ytrainS = MNIST_ytrain_res\n",
        "XtestS = MNIST_Xtest\n",
        "ytestS = MNIST_ytest\n",
        "\n",
        "# # Target domain\n",
        "XtrainT = USPS_Xtrain_res\n",
        "ytrainT = USPS_ytrain_res\n",
        "XtestT = USPS_Xtest\n",
        "ytestT = USPS_ytest\n",
        "\n",
        "# Target domain\n",
        "# XtrainT = SVHN_Xtrain_res\n",
        "# ytrainT = SVHN_ytrain_res\n",
        "# XtestT = SVHN_Xtest\n",
        "# ytestT = SVHN_ytest\n",
        "\n",
        "Ns=1000 # number of samples in source domain\n",
        "Nt=1000 # number of samples in target domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUK-KBEHE8ls"
      },
      "source": [
        "Implement the method following the description of the method in the slides of the lecture (around slide 75).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEJOXzpn0HFU"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def SA(XTrainS,XTestS,XTrainT,XTestT,d=10):\n",
        "  \"\"\"\n",
        "  Subspace Alignment (SA)\n",
        "\n",
        "  Input\n",
        "  XTrainS : Input training source data [NTrS,D]\n",
        "  XTestS : Input test source data [NTeS,D]\n",
        "  XTrainT : Input training target data [NTrT,D]\n",
        "  XTestT : Input test target data [NTeT,D]\n",
        "  d : number of PCA components\n",
        "\n",
        "  Output\n",
        "  XTrainSp : projected and transformed source training data [NTrS,d]\n",
        "  XTestSp : projected and transformed source test data [NTeS,d]\n",
        "  XTrainTp : projected training target data [NTrT,d]\n",
        "  XTestTp : projected test target data [NTeT,d]\n",
        "  \"\"\"\n",
        "\n",
        "XXXXXXXXX\n",
        "\n",
        "  return XTrainSp,XTestSp,XTrainTp,XTestTp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQRG3acnpP7f"
      },
      "source": [
        "We start by veryfying the score of a classifier trained on the source domain and tested on the target domain (no adaptation).\n",
        "\n",
        "You can use the classifier you want (e.g., logistic regression, SVM, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQLUXx6OOOIO"
      },
      "outputs": [],
      "source": [
        "XXXXXXX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzTyWoM9pcYu"
      },
      "source": [
        "To reduce dimensionality, we can also first compute PCA using the source training set and then project (i.e., transform) all data (source and target, training and test) on the first $d$ components.\n",
        "\n",
        "**Question**: try different values of $d$. Is there a difference with the previous full-training data appraoch ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7fB-CyyoqnN"
      },
      "outputs": [],
      "source": [
        "XXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kzFJD8gqC0T"
      },
      "source": [
        "We can now test the SA approach.\n",
        "\n",
        "**Question**: try different values of $d$. Does the performance improve on the target domain ? Try at least two different ML methods (one linear and one non-linear) and one DL architecture (it can be a pre-trained model or a simple architecture). What's the best ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydAQ2eF_b0tZ"
      },
      "outputs": [],
      "source": [
        "XXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKXSjtgOCtCM"
      },
      "source": [
        "**Question**: Use the UMAP or t-SNE visualization to check whether the data are more similar after the SA adaptation. What can you conclude ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tmZN_4I_dRF"
      },
      "outputs": [],
      "source": [
        "XXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7MHgiQyDG9n"
      },
      "source": [
        "**Question**: Repeat the same experiments for different Source and Target domains. Use at least the two followng configurations:\n",
        "\n",
        "Source: MNIST, Target: USPS\n",
        "\n",
        "Source MNIST, Target: SVHN\n",
        "\n",
        "**Queston**: Comment the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED1Uff9PDY8h"
      },
      "source": [
        "**Question**: What does it happen if you decrease the number of samples for the labeled training set? Is it expected based on the theory seen during the lecture ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV6UCi8JDpfD"
      },
      "source": [
        "**Optional**: You can test other methods using the `adapt`library. See `https://adapt-python.github.io/adapt/`. You will need Tensorflow==2.15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyvAkuA5Dxj8"
      },
      "source": [
        "Here, you can test the MDD and DANN methods using a very simple architecture for both the encoder and discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCYqGRCbJm2U"
      },
      "outputs": [],
      "source": [
        "from adapt.feature_based import MDD\n",
        "model = MDD(encoder=XXXX,task=XXXXX,lambda_=0.1, gamma=4., Xt=XtrainT, metrics=[\"accuracy\"], random_state=0)\n",
        "model.fit(XtrainS, ytrainS, epochs=100, verbose=0)\n",
        "model.score(XtrainS, ytrainS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHCz-PVCQSza"
      },
      "outputs": [],
      "source": [
        "from adapt.feature_based import DANN\n",
        "\n",
        "model = DANN(XXXXXX)\n",
        "model.fit(XXXXXX)\n",
        "model.score(XXXXXXX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huyz-pEBRK6x"
      },
      "source": [
        "\n",
        "**Question (Optional)**: try to use a more complex architecture (for instance a pre-trained model) for both the encoder and discriminator. Be careful, the adapt library is written in TensorFlow..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "5e59cd04a9cdc53c5128ea57bd8f9b712fba8f1ae7bfd07c006521541be527e5"
    },
    "kernelspec": {
      "display_name": "Python 3.11.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
